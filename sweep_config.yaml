program: train.py
method: bayes
metric:
  goal: maximize
  name: policy_stats/avg_true_objective

env:
  PYTHONPATH: /workspace/metta

parameters:
  train_for_seconds:
    values:
      - 108
    distribution: categorical

  restart_behavior:
    values:
      - restart
    distribution: categorical

  with_wandb:
    values:
      - True
    distribution: categorical

  algo:
    values:
      - APPO
    distribution: categorical

  env:
    values:
      - GDY-PowerGrid
    distribution: categorical

  num_workers:
    values:
      - 25
    distribution: categorical

  decorrelate_experience_max_seconds:
    max: 200
    min: 1
    distribution: int_uniform

  env_num_agents:
    values:
      - 20
    distribution: categorical

  env_width:
    values:
      - 20:100
    distribution: categorical

  env_height:
    values:
      - 20:100
    distribution: categorical

  num_batches_to_accumulate:
    max: 4
    min: 1
    distribution: int_uniform
  exploration_loss_coeff:
    max: 0.006
    min: 0.0015
    distribution: uniform
  policy_initialization:
    values:
      - orthogonal
      - xavier_uniform
    distribution: categorical
  num_batches_per_epoch:
    max: 8
    min: 1
    distribution: int_uniform
  value_loss_coeff:
    max: 1
    min: 0.25
    distribution: uniform
  exploration_loss:
    values:
      - entropy
      - symmetric_kl
    distribution: categorical
  value_bootstrap:
    values:
      - "true"
      - "false"
    distribution: categorical
  agent_fc_layers:
    max: 12
    min: 1
    distribution: int_uniform
  rnn_num_layers:
    max: 3
    min: 1
    distribution: int_uniform
  max_policy_lag:
    max: 2000
    min: 500
    distribution: int_uniform
  initial_stddev:
    max: 2
    min: 1
    distribution: int_uniform
  aux_loss_coeff:
    max: 0.2
    min: 0
    distribution: uniform
  learning_rate:
    max: 0.0001
    min: 0.000005
    distribution: uniform
  agent_fc_size:
    values:
      - 256
      - 512
      - 1024
      - 2048
    distribution: categorical
  nonlinearity:
    values:
      - elu
      - relu
    distribution: categorical
  batch_size:
    values:
      - 256
      - 512
      - 1024
      - 2048
    distribution: categorical
  rollout:
    values:
      - 8
      - 16
      - 32
      - 64
      - 128
      - 256
      - 512
    distribution: categorical
  rnn_type:
    values:
      - gru
      - lstm
    distribution: categorical
  rnn_size:
    values:
      - 256
      - 512
      - 1024
      - 2048
    distribution: categorical
