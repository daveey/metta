### PufferLib demo environments
# Package parameters override defaults.
# Parameters for specific envs override packages
# You cannot specify any deeper than that.
default:
  package: ~
  env_name: ~
  env: {}
  policy: {}
  use_rnn: False
  rnn: {}
  train:
    seed: 1
    torch_deterministic: True
    cpu_offload: False
    device: cpu
    total_timesteps: 10_000_000
    learning_rate: 2.5e-4
    num_steps: 128
    anneal_lr: True
    gamma: 0.99
    gae_lambda: 0.95
    num_minibatches: 4
    update_epochs: 4
    norm_adv: True
    clip_coef: 0.1
    clip_vloss: True
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 0.5
    target_kl: ~

    num_envs: 8
    num_workers: 8
    env_batch_size: ~
    zero_copy: True
    verbose: True
    data_dir: experiments
    checkpoint_interval: 200
    batch_size: 1024
    minibatch_size: 512
    bptt_horizon: 16
    vf_clip_coef: 0.1
    compile: False
    compile_mode: reduce-overhead

  sweep:
    method: random
    name: sweep
    metric:
      goal: maximize
      name: environment/episode_return
    # Nested parameters name required by WandB API
    parameters:
      train:
        parameters:
          learning_rate: {
            'distribution': 'log_uniform_values',
            'min': 1e-4,
            'max': 1e-1,
          }
          batch_size: {
            'values': [512, 1024, 2048],
          }
          minibatch_size: {
            'values': [128, 256, 512],
          }
          bptt_horizon: {
            'values': [4, 8, 16],
          }

metta:
  package: metta
  env_name: mettagrid
  train:
    total_timesteps: 50_000_000
    learning_rate: 0.00175
    num_envs: 1
    num_workers: 1
    env_batch_size: 1
    update_epochs: 1
    gamma: 0.925
    gae_lambda: 0.99
    ent_coef: 0.0018
    bptt_horizon: 8
    batch_size: 65536
    minibatch_size: 2048
    compile: False
    anneal_lr: False
  sweep:
    method: bayes
    name: sweep
    metric:
      goal: maximize
      name: environment/episode_return
    # Nested parameters name required by WandB API
    parameters:
      train:
        parameters:
          ent_coef: {
            'distribution': 'log_uniform_values',
            'min': 1e-3,
            'max': 5e-2,
          }
          gamma: {
            'values': [0.90, 0.925, 0.95, 0.975, 0.99],
          }
          gae_lambda: {
            'values': [0.90, 0.925, 0.95, 0.975, 0.99],
          }
          learning_rate: {
            'distribution': 'log_uniform_values',
            'min': 1e-4,
            'max': 1e-1,
          }
          batch_size: {
            'values': [4096, 8192, 16384, 32768, 65536],
          }
          minibatch_size: {
            'values': [1024, 2048, 4096, 8192, 16384],
          }
          bptt_horizon: {
            'values': [1, 2, 4, 8, 16],
          }
